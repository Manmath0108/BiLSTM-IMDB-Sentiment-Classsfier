{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7deea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manmath/miniconda3/envs/nlp_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Seed:  42\n",
      "Max Length:  256\n",
      "Batch Size:  32\n",
      "Tokenizer:  distilbert-base-uncased\n",
      "Torch Version:  2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "SEED = 42\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Device: \", DEVICE)\n",
    "print(\"Seed: \", SEED)\n",
    "print(\"Max Length: \", MAX_LEN)\n",
    "print(\"Batch Size: \", BATCH_SIZE)\n",
    "print(\"Tokenizer: \", MODEL_NAME)\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87da5714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['review', 'sentiment'], dtype='object')\n",
      "Initial size:  50000\n",
      "Label Distribution\n",
      "sentiment\n",
      "1    25000\n",
      "0    25000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train samples: 40000\n",
      "Test samples: 10000\n",
      "\n",
      "Sample Example: \n",
      "{'text': \"I caught this little gem totally by accident back in 1980 or '81. I was at a revival theatre to see two old silly sci-fi movies. The theatre was packed full and (with no warning) they showed a bunch o...\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "CSV_PATH = \"/home/manmath/Desktop/MyProjects/BiLSTM-IMDB-Sentiment-Classsfier/Data/IMDB Dataset.csv\"\n",
    "TEXT_COL = \"review\"\n",
    "LABEL_COL = \"sentiment\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(\"Columns:\", df.columns)\n",
    "print(\"Initial size: \", len(df))\n",
    "\n",
    "df = df[[TEXT_COL, LABEL_COL]].dropna()\n",
    "\n",
    "df[LABEL_COL] = df[LABEL_COL].map({\n",
    "    \"positive\":1,\n",
    "    \"negative\":0\n",
    "})\n",
    "\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
    "\n",
    "print(\"Label Distribution\")\n",
    "print(df[LABEL_COL].value_counts())\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[LABEL_COL]\n",
    ")\n",
    "\n",
    "train_texts = train_df[TEXT_COL].tolist()\n",
    "train_labels = train_df[LABEL_COL].tolist()\n",
    "\n",
    "test_texts = test_df[TEXT_COL].tolist()\n",
    "test_labels = test_df[LABEL_COL].tolist()\n",
    "\n",
    "print(\"\\nTrain samples:\", len(train_texts))\n",
    "print(\"Test samples:\", len(test_texts))\n",
    "\n",
    "print(\"\\nSample Example: \")\n",
    "print({\n",
    "    \"text\": train_texts[0][:200] + \"...\",\n",
    "    \"label\": train_labels[0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa726ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size:  30522\n",
      "Batch Keys: KeysView({'input_ids': tensor([[ 101, 2568, 2017,  ..., 2007, 2784,  102],\n",
      "        [ 101, 1045, 2228,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 2003,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2023, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2026, 6898,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 3185,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 1])})\n",
      "input_ids shape: torch.Size([32, 256])\n",
      "attention_mask shape: torch.Size([32, 256])\n",
      "labels shape: torch.Size([32])\n",
      "First 8 Labels:  tensor([0, 0, 1, 1, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "print(\"Tokenizer vocab size: \", len(tokenizer.get_vocab()))\n",
    "\n",
    "class IMDBHFDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length: int = MAX_LEN):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = [int(i) for i in labels]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        txt = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            txt, \n",
    "            truncation = True,\n",
    "            max_length = self.max_length,\n",
    "            padding = False,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = None\n",
    "        )\n",
    "\n",
    "        encoded[\"labels\"] = label\n",
    "        return encoded\n",
    "\n",
    "collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "train_dataset = IMDBHFDataset(train_texts, train_labels, tokenizer, max_length=MAX_LEN)\n",
    "test_dataset = IMDBHFDataset(test_texts, test_labels, tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch Keys:\", batch.keys())\n",
    "print(\"input_ids shape:\", batch[\"input_ids\"].shape)\n",
    "print(\"attention_mask shape:\", batch[\"attention_mask\"].shape)\n",
    "print(\"labels shape:\", batch[\"labels\"].shape)\n",
    "print(\"First 8 Labels: \", batch[\"labels\"][:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db6162d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input_ids shape:  torch.Size([32, 256])\n",
      "Logits Shape:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=200, hidden_dim=128, num_layers=1, pad_idx=0, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        emb = self.embedding(input_ids)\n",
    "\n",
    "        outputs, (h_n, c_n) = self.lstm(emb)\n",
    "\n",
    "        forward_final = h_n[-2]\n",
    "        backward_final = h_n[-1]\n",
    "\n",
    "        hidden = torch.cat([forward_final, backward_final], dim=1)\n",
    "        x = self.dropout(hidden)\n",
    "\n",
    "        logits = self.fc(x).squeeze(-1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "pad_id = tokenizer.pad_token_id or 0\n",
    "\n",
    "model = BiLSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=1,\n",
    "    pad_idx=pad_id,\n",
    "    dropout=0.3,\n",
    ").to(DEVICE)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask)\n",
    "\n",
    "print(\"Batch input_ids shape: \", input_ids.shape)\n",
    "print(\"Logits Shape: \", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe1286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Train batches: 1250, Test batches: 313\n",
      "  step 0100  avg_loss=0.6913  acc=0.5294\n",
      "  step 0200  avg_loss=0.6875  acc=0.5475\n",
      "  step 0300  avg_loss=0.6822  acc=0.5602\n",
      "  step 0400  avg_loss=0.6769  acc=0.5754\n",
      "  step 0500  avg_loss=0.6728  acc=0.5839\n",
      "  step 0600  avg_loss=0.6668  acc=0.5919\n",
      "  step 0700  avg_loss=0.6617  acc=0.6017\n",
      "  step 0800  avg_loss=0.6597  acc=0.6042\n",
      "  step 0900  avg_loss=0.6630  acc=0.5978\n",
      "  step 1000  avg_loss=0.6643  acc=0.5954\n",
      "  step 1100  avg_loss=0.6630  acc=0.5971\n",
      "  step 1200  avg_loss=0.6571  acc=0.6043\n",
      "Epoch 01  TrainLoss=0.6510  TrainAcc=0.6105  ValLoss=0.5281  ValAcc=0.7541  Time=64.5s\n",
      "  -> Saved best model to bilstm_imdb_best.pt\n",
      "  step 0100  avg_loss=0.5401  acc=0.7178\n",
      "  step 0200  avg_loss=0.5447  acc=0.7166\n",
      "  step 0300  avg_loss=0.5418  acc=0.7189\n",
      "  step 0400  avg_loss=0.5222  acc=0.7340\n",
      "  step 0500  avg_loss=0.5034  acc=0.7514\n",
      "  step 0600  avg_loss=0.4853  acc=0.7646\n",
      "  step 0700  avg_loss=0.4711  acc=0.7744\n",
      "  step 0800  avg_loss=0.4580  acc=0.7829\n",
      "  step 0900  avg_loss=0.4501  acc=0.7885\n",
      "  step 1000  avg_loss=0.4420  acc=0.7936\n",
      "  step 1100  avg_loss=0.4398  acc=0.7962\n",
      "  step 1200  avg_loss=0.4354  acc=0.7997\n",
      "Epoch 02  TrainLoss=0.4325  TrainAcc=0.8015  ValLoss=0.3564  ValAcc=0.8485  Time=65.9s\n",
      "  -> Saved best model to bilstm_imdb_best.pt\n",
      "  step 0100  avg_loss=0.2772  acc=0.8844\n",
      "  step 0200  avg_loss=0.2849  acc=0.8844\n",
      "  step 0300  avg_loss=0.2819  acc=0.8846\n",
      "  step 0400  avg_loss=0.2829  acc=0.8859\n",
      "  step 0500  avg_loss=0.2857  acc=0.8842\n",
      "  step 0600  avg_loss=0.2844  acc=0.8846\n",
      "  step 0700  avg_loss=0.2839  acc=0.8849\n",
      "  step 0800  avg_loss=0.2851  acc=0.8849\n",
      "  step 0900  avg_loss=0.2851  acc=0.8853\n",
      "  step 1000  avg_loss=0.2843  acc=0.8855\n",
      "  step 1100  avg_loss=0.2845  acc=0.8856\n",
      "  step 1200  avg_loss=0.2843  acc=0.8859\n",
      "Epoch 03  TrainLoss=0.2848  TrainAcc=0.8855  ValLoss=0.3057  ValAcc=0.8737  Time=66.0s\n",
      "  -> Saved best model to bilstm_imdb_best.pt\n",
      "  step 0100  avg_loss=0.2028  acc=0.9266\n",
      "  step 0200  avg_loss=0.2129  acc=0.9213\n",
      "  step 0300  avg_loss=0.2134  acc=0.9201\n",
      "  step 0400  avg_loss=0.2154  acc=0.9189\n",
      "  step 0500  avg_loss=0.2181  acc=0.9185\n",
      "  step 0600  avg_loss=0.2153  acc=0.9188\n",
      "  step 0700  avg_loss=0.2158  acc=0.9185\n",
      "  step 0800  avg_loss=0.2145  acc=0.9189\n",
      "  step 0900  avg_loss=0.2121  acc=0.9202\n",
      "  step 1000  avg_loss=0.2127  acc=0.9202\n",
      "  step 1100  avg_loss=0.2125  acc=0.9203\n",
      "  step 1200  avg_loss=0.2117  acc=0.9206\n",
      "Epoch 04  TrainLoss=0.2118  TrainAcc=0.9204  ValLoss=0.3102  ValAcc=0.8749  Time=64.7s\n",
      "  step 0100  avg_loss=0.1382  acc=0.9544\n",
      "  step 0200  avg_loss=0.1443  acc=0.9519\n",
      "  step 0300  avg_loss=0.1438  acc=0.9523\n",
      "  step 0400  avg_loss=0.1468  acc=0.9506\n",
      "  step 0500  avg_loss=0.1472  acc=0.9498\n",
      "  step 0600  avg_loss=0.1468  acc=0.9499\n",
      "  step 0700  avg_loss=0.1484  acc=0.9490\n",
      "  step 0800  avg_loss=0.1496  acc=0.9484\n",
      "  step 0900  avg_loss=0.1493  acc=0.9486\n",
      "  step 1000  avg_loss=0.1489  acc=0.9488\n",
      "  step 1100  avg_loss=0.1496  acc=0.9486\n",
      "  step 1200  avg_loss=0.1499  acc=0.9481\n",
      "Epoch 05  TrainLoss=0.1497  TrainAcc=0.9481  ValLoss=0.3414  ValAcc=0.8746  Time=65.2s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "EPOCHS = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_path = \"bilstm_imdb_best.pt\"\n",
    "\n",
    "print(f\"Training on device: {DEVICE}\")\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # -------- TRAIN LOOP --------\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].float().to(DEVICE)   # shape [B]\n",
    "\n",
    "        logits = model(input_ids, attention_mask)     # [B]\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "        correct += (preds == labels.long()).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            avg_loss = running_loss / total\n",
    "            acc = correct / total\n",
    "            print(f\"  step {step:04d}  avg_loss={avg_loss:.4f}  acc={acc:.4f}\")\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # -------- VALIDATION LOOP --------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].float().to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            val_correct += (preds == labels.long()).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_loss = val_loss / val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  TrainLoss={train_loss:.4f}  TrainAcc={train_acc:.4f}  \"\n",
    "          f\"ValLoss={val_loss:.4f}  ValAcc={val_acc:.4f}  Time={dt:.1f}s\")\n",
    "\n",
    "    # save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "        print(f\"  -> Saved best model to {best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b0892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
